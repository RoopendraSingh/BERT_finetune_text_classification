{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model with fine tuning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### for importing the packages #####\n",
    "#!pip3 install transformers\n",
    "#!pip3 install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import calendar\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import classification_report,f1_score,precision_score,recall_score,confusion_matrix,accuracy_score\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import Train, Validation and Test dataset respectively\n",
    "df1 = pd.read_csv('GL_subtypes_data/Train_data_GL.csv')\n",
    "df2 = pd.read_csv('GL_subtypes_data/Val_data_GL.csv')\n",
    "df3 = pd.read_csv('GL_subtypes_data/Test_data_GL.csv')\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Prepare dataset to be given as input to bert model\n",
    "# Function for preparing Training , Validation & Test dataset for model input\n",
    "def get_embeddings(df):\n",
    "    label_counts = pd.DataFrame(df['Label'].value_counts())\n",
    "    label_values = list(label_counts.index)\n",
    "    df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "    # Get the lists of sentences and their labels.\n",
    "    texts = df.Text.values\n",
    "    labels = df.Label.values\n",
    "    \n",
    "    text_lengths = [len(texts[i].split()) for i in range(len(texts))]\n",
    "    \n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    # For every sentence...\n",
    "    for sent in texts:\n",
    "       # encode` will:\n",
    "       # (1) Tokenize the sentence.\n",
    "       # (2) Prepend the `[CLS]` token to the start. special classification token\n",
    "       # (3) Append the `[SEP]` token to the end. special token\n",
    "       # (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, \n",
    "                            #max_length = 128,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_sent)\n",
    "    \n",
    "    text_ids_lengths = [len(input_ids[i]) for i in range(len(input_ids))]\n",
    "\n",
    "    # We'll borrow the `pad_sequences` utility function to do this.\n",
    "    #from keras.preprocessing.sequence import pad_sequences\n",
    "    # Set the maximum sequence length.\n",
    "    # I've chosen 145 somewhat arbitrarily. It's slightly larger than the\n",
    "    # maximum training sentence length of 118...\n",
    "    MAX_LEN = 145\n",
    "\n",
    "    # Pad our input tokens with value 0.\n",
    "    # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "    # as opposed to the beginning.\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Create attention masks\n",
    "    att_masks = []\n",
    "\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        # Create the attention mask.\n",
    "        #- If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #- If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        # Store the attention mask for this sentence.\n",
    "        att_masks.append(att_mask)\n",
    "    print('Done!')   \n",
    "    return input_ids, labels, att_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, train_m = get_embeddings(df1)\n",
    "val_x, val_y, val_m = get_embeddings(df2)\n",
    "test_x, test_y, test_m = get_embeddings(df3)\n",
    "\n",
    "\n",
    "# Our model expects PyTorch tensors rather than numpy.ndarrays\n",
    "# convert arrays to tensors\n",
    "train_x = torch.tensor(train_x)\n",
    "test_x = torch.tensor(test_x)\n",
    "val_x = torch.tensor(val_x)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_y = torch.tensor(test_y)\n",
    "val_y = torch.tensor(val_y)\n",
    "train_m = torch.tensor(train_m)\n",
    "test_m = torch.tensor(test_m)\n",
    "val_m = torch.tensor(val_m)\n",
    "\n",
    "print(train_x.shape,train_y.shape,train_m.shape)\n",
    "print(test_x.shape,test_y.shape,test_m.shape)\n",
    "print(val_x.shape,val_y.shape,val_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll also create an iterator for our dataset using the torch DataLoader class\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_x, train_m, train_y)\n",
    "# define samplers for obtaining training batches\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# load training data in batches\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_x, val_m, val_y)\n",
    "# define samplers for obtaining validation batches\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# load validation data in batches\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# getting the number of classes\n",
    "num_labels = len(set(df1.Label))\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "# Use the 12-layer BERT model, with an uncased vocab.\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = num_labels, output_attentions = False, output_hidden_states = False)\n",
    "\n",
    "#Get the GPU to be used else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "##### count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of trainable parameters:', count_parameters(model), '\\n', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "#The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.2},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "\n",
    "num_epochs = 6\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "seed_val = 111\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Run this cell for training the model ######\n",
    "\n",
    "# to track the training loss as the model trains\n",
    "train_losses = []\n",
    "# to track the validation loss as the model trains\n",
    "val_losses = []\n",
    "val_losses1 = []\n",
    "num_mb_train = len(train_dataloader)\n",
    "num_mb_val = len(val_dataloader)\n",
    "\n",
    "if num_mb_val == 0:\n",
    "    num_mb_val = 1\n",
    "    \n",
    "for n in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "    for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # prep model for training\n",
    "        model.train()\n",
    "        \n",
    "        mb_x = mb_x.to(device)\n",
    "        mb_m = mb_m.to(device)\n",
    "        mb_y = mb_y.to(device)\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "        # calculate the loss\n",
    "        loss = outputs[0]\n",
    "        #loss = model_loss(outputs[1], mb_y)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.data / num_mb_train\n",
    "    \n",
    "    print (\"\\nTrain loss after itaration %i: %f\" % (n+1, train_loss))\n",
    "    #record training loss\n",
    "    train_losses.append(train_loss.cpu())\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "    with torch.no_grad():\n",
    "        # prep model for evaluation\n",
    "        model.eval()\n",
    "        \n",
    "        for k, (mb_x, mb_m, mb_y) in enumerate(val_dataloader):\n",
    "            mb_x = mb_x.to(device)\n",
    "            mb_m = mb_m.to(device)\n",
    "            mb_y = mb_y.to(device)\n",
    "        \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
    "            # calculate the loss\n",
    "            loss = outputs[0]\n",
    "            #loss = model_loss(outputs[1], mb_y)\n",
    "            \n",
    "            val_loss += loss.data / num_mb_val\n",
    "            \n",
    "        print (\"Validation loss after itaration %i: %f\" % (n+1, val_loss))\n",
    "        # record validation loss\n",
    "        val_losses.append(val_loss.cpu())\n",
    "        \n",
    "        ###### early stopping #####\n",
    "        val_loss1=float(val_loss.cpu())\n",
    "        val_losses1.append(val_loss1)\n",
    "        best_loss=min(val_losses1)\n",
    "        if val_loss1 <= best_loss:\n",
    "            best_loss = val_loss\n",
    "            check_without_progress = 0\n",
    "        else:\n",
    "            check_without_progress +=1\n",
    "            if check_without_progress >= 3:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Time: {epoch_mins}m {epoch_secs}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### SAVE THE MODEL ######\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "out_dir = './subtype_bert_EO_prop_merge_upsampled_final'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir + '/train_losses.pkl', 'wb') as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "    \n",
    "with open(out_dir + '/val_losses.pkl', 'wb') as f:\n",
    "    pickle.dump(val_losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### LOAD THE MODEL ###### only to be run when model is saved\n",
    "import pickle\n",
    "import os\n",
    "out_dir = './subtype_bert_EO_prop_merge_final'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(out_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "with open(out_dir + '/train_losses.pkl', 'rb') as f:\n",
    "    train_losses = pickle.load(f)\n",
    "    \n",
    "with open(out_dir + '/val_losses.pkl', 'rb') as f:\n",
    "    val_losses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the results and analyzing the metrics ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the train loss as the network trained\n",
    "plt.figure()\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the validation loss as the network trained\n",
    "plt.figure()\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "test_data = TensorDataset(test_x, test_m)\n",
    "# define samplers for obtaining test batches\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "# load test data in batches\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    model.eval()# prep model for evaluation\n",
    "    for k, (mb_x, mb_m) in enumerate(test_dataloader):\n",
    "        mb_x = mb_x.to(device)\n",
    "        mb_m = mb_m.to(device)\n",
    "        # get sample outputs\n",
    "        output = model(mb_x, attention_mask=mb_m)\n",
    "        outputs.append(output[0].to('cpu'))\n",
    "        \n",
    "outputs = torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output logits to predicted class\n",
    "_, predicted_values = torch.max(outputs, 1)\n",
    "predicted_values = predicted_values.numpy()\n",
    "true_values = test_y.numpy()\n",
    "\n",
    "# calculate test accuracy\n",
    "test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n",
    "print (\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(true_values, predicted_values,average='macro'))\n",
    "print(precision_score(true_values, predicted_values,average='macro'))\n",
    "print(recall_score(true_values, predicted_values,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(recall_score(true_values, predicted_values,average=None)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "# code borrowed from scikit-learn.org\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=list(np.unique(df1.Label))\n",
    "total_labels=len(np.unique(df1.Label))\n",
    "\n",
    "if total_labels==19:\n",
    "    #### Labels for Property\n",
    "    labels = [\"Acc\",\"Acc_Clean\",\"Acc_Install\",\"Acc_Mowing\",\"BOC_Work\",\"Data/Cyber\",\"Equip\",\"Equip/Water\",\n",
    "              \"Equip/drop\",\"Fire\",\"Misc\",\"Missing\",\"Natural\",\"Theft\",\"Unknown\",\"Vehicle\",\"Water\",\"Water/leak\",\"Water/pipe\"]\n",
    "elif total_labels==16:\n",
    "    #### Labels For GL\n",
    "     labels = [\"Acc\",\"Acc_Clean\",\"Acc_Install\",\"Acc_Mowing\",\"BI-Slip/Fall\",\"BI-others\",\"BOC/Work\",\"Equip\",\"Equip/drop\",\n",
    "            \"Fire\",\"Misc\",\"Theft/Missing\",\"Unknown\",\"Vehicle\",\"Water\",\"Water/leak\"]\n",
    "elif total_labels==18:             \n",
    "    #### Labels for E&O\n",
    "    labels = [\"BOC\",\"BI\",\"Copyright\",\"Data_Cyber\",\"Disclose\",\"Disc_Miscon\",\"Financial\",\"Fin_Debt_Cr\",\"Fin_For\",\"Fraud_Misrep\",\n",
    "              \"Misc\",\"Misrep\",\"Negl\",\"Negl_Def_Const\",\"Neg_Tax\",\"Property\",\"Unknown\",\"Work\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(true_values, predicted_values)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure(figsize=(9,9))\n",
    "plot_confusion_matrix(cm_test, classes=labels, title='Confusion Matrix - Test Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicted Dataset ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred=pd.DataFrame({\"Text\":df3.Text,\"Prediction\":predicted_values})\n",
    "data_pred[\"Prediction\"]= data_pred[\"Prediction\"].replace(num_labels,labels)\n",
    "data_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLASSIFICATION MODEL \"TEST & USE\" and \"INTERPRETATION\" #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample test data for interpretability\n",
    "def get_test_data(docs):\n",
    "    \n",
    "    text_lengths = [len(docs[i].split()) for i in range(len(docs))]\n",
    "    input_ids = []\n",
    "\n",
    "    for sent in docs:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, \n",
    "                            #max_length = 128,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        input_ids.append(encoded_sent)\n",
    "    \n",
    "    text_ids_lengths = [len(input_ids[i]) for i in range(len(input_ids))]\n",
    "\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    MAX_LEN = 145\n",
    "\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # Create attention masks\n",
    "    att_masks = []\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        att_masks.append(att_mask)\n",
    "    print('Done!')   \n",
    "    return input_ids, att_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction outputs(probabilities) on test data set to be used for interpretation\n",
    "def prediction(docs):\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    test_x, test_m = get_test_data(docs)\n",
    "    test_x = torch.tensor(test_x)\n",
    "    test_m = torch.tensor(test_m)    \n",
    "    test_data = TensorDataset(test_x, test_m)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for k, (mb_x, mb_m) in enumerate(test_dataloader):\n",
    "            mb_x = mb_x.to(device)\n",
    "            mb_m = mb_m.to(device)\n",
    "            output = model(mb_x, attention_mask=mb_m)\n",
    "            outputs.append(output[0].to('cpu'))\n",
    "\n",
    "    outputs = torch.cat(outputs)\n",
    "    outputs = torch.nn.functional.softmax(outputs)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get test dataset\n",
    "df3 = pd.read_csv('Property Datasets/Simple_Data/Test_data_property.csv')\n",
    "df3['Label'] = df3['Label'].astype(int)\n",
    "test_texts = df3.Text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = prediction(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels..\n",
    "PD_labels = [\"BOC/Work\",\"Data/Cyber\",\"Equipment\",\"Fire\",\"Misc\",\"Natural\",\"Property\",\"Theft/Missing\",\"Unknown\",\"Vehicle\",\"Water\"]\n",
    "GL_labels=[\"Accidental\",\"BI-Others\",\"BI-Slip/Fall\",\"BOC/Work\",\"Equipment\",\"Fire\",\"Miscellaneous\",\"Theft/Missing\",\"Unknown\",\"Vehicle\",\"Water\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "te = TextExplainer(random_state=2019)\n",
    "te.fit(test_texts[1], prediction)\n",
    "te.show_prediction(target_names=PD_labels,top=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=PD_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_texts[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_texts[19])\n",
    "exp = explainer.explain_instance(test_texts[19], prediction, num_features=5, top_labels=4)\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### For Saving File #######\n",
    "exp.save_to_file('lime1_PD2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://OCR_cloud_vision_API.ipynb [Content-Type=application/octet-stream]...\n",
      "/ [1/1 files][  7.6 KiB/  7.6 KiB] 100% Done                                    \n",
      "Operation completed over 1 objects/7.6 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r \"OCR_cloud_vision_API.ipynb\" \"gs://sftp-uploaded-files/BERT_final_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
